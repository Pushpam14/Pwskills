{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : Explain the difference between simple linear regression and multiple linear regression. Provide an example of each\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear regression is a statistical technique used to analyze the relationship between two variables, where one variable (known as the independent or predictor variable) is used to predict the value of another variable (known as the dependent or response variable). Simple linear regression involves fitting a straight line to a set of data points, and the equation of this line is used to make predictions. For example, we might use simple linear regression to predict a person's weight (dependent variable) based on their height (independent variable).\n",
    "\n",
    "### Equation for simple linear regression:\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{y}$ is the predicted value of the dependent variable\n",
    "* x is the value of the independent variable for which we want to make a prediction\n",
    "* $\\beta_0$ is the intercept (the value of y when x=0)\n",
    "* $\\beta_1$ is the slope (the change in y for a unit change in x)\n",
    "\n",
    "### Multiple linear regression, on the other hand, is a statistical technique used to analyze the relationship between more than two variables. It involves fitting a linear equation to a set of data points that includes multiple independent variables, and using this equation to make predictions. For example, we might use multiple linear regression to predict a person's salary (dependent variable) based on their age, education level, and years of experience (independent variables).\n",
    "\n",
    "### Equation for Multiple Linear Regression :\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{y}$ is the predicted value of the dependent variable\n",
    "* $x_1$, $x_2$, ..., $x_k$ are the values of the independent variables for which we want to make a prediction\n",
    "* $\\beta_0$ is the intercept (the value of y when all independent variables are 0)\n",
    "* $\\beta_1$, $\\beta_2$, ..., $\\beta_k$ are the slopes (the change in y for a unit change in each independent variable) for each independent variable $x_1$, $x_2$, ..., $x_k$, respectively.\n",
    "\n",
    "### To illustrate the difference between these two techniques, consider the following examples:\n",
    "\n",
    "1. Example of Simple Linear Regression: Suppose we want to determine whether there is a relationship between the amount of time (in minutes) a person spends exercising each day (independent variable) and their resting heart rate (dependent variable). We collect data on 50 individuals, including their daily exercise time and their resting heart rate, and plot the data points on a scatter plot. We can then fit a straight line to the data points that best represents the relationship between exercise time and resting heart rate. We can use this line to predict the resting heart rate of individuals based on the amount of time they spend exercising each day.\n",
    "\n",
    "2. Example of Multiple Linear Regression: Suppose we have a dataset that includes the salaries (dependent variable) of 100 employees, along with their age, education level, and years of experience (independent variables). We can use multiple linear regression to determine how well age, education level, and years of experience predict salary. We can fit a linear equation that includes all three independent variables to the data points, and use this equation to make predictions about the salary of new employees based on their age, education level, and years of experience."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 : Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression is a commonly used statistical technique to model the relationship between a dependent variable and one or more independent variables. However, the validity of the results obtained through linear regression depends on several assumptions. Violation of these assumptions may lead to inaccurate or biased results. The key assumptions of linear regression are:\n",
    "\n",
    "1. Linearity: The relationship between the dependent variable and the independent variable(s) should be linear. This means that the change in the dependent variable should be proportional to the change in the independent variable(s).\n",
    "\n",
    "2. Independence: The observations should be independent of each other, meaning that the value of the dependent variable for one observation should not be related to the value of the dependent variable for any other observation.\n",
    "\n",
    "3. Homoscedasticity: The variance of the errors (the difference between the observed values and the predicted values) should be constant across all levels of the independent variable(s). This is known as homoscedasticity.\n",
    "\n",
    "4. Normality: The errors should be normally distributed, meaning that the frequency distribution of the errors should be bell-shaped.\n",
    "\n",
    "5. No multicollinearity: In multiple linear regression, there should be no perfect linear relationship among the independent variables.\n",
    "\n",
    "### To check whether these assumptions hold in a given dataset, several diagnostic tests can be performed:\n",
    "\n",
    "1. Scatter plot: A scatter plot can be used to visualize the relationship between the dependent variable and each independent variable. A linear relationship should be visible.\n",
    "\n",
    "2. Residual plot: A plot of the residuals (the difference between the observed values and the predicted values) against the predicted values can be used to check for homoscedasticity. If the variance of the residuals is constant across all levels of the independent variable(s), then the assumption of homoscedasticity holds.\n",
    "\n",
    "3. Normal probability plot: A normal probability plot of the residuals can be used to check for normality. If the points on the plot follow a straight line, then the assumption of normality holds.\n",
    "\n",
    "4. Variance Inflation Factor (VIF): VIF can be calculated to detect multicollinearity in multiple linear regression. If VIF is greater than 5, then multicollinearity is a concern.\n",
    "\n",
    "5. Durbin-Watson test: The Durbin-Watson test can be used to check for autocorrelation (dependence between the residuals). If the test statistic falls between 0 and 4, then the assumption of independence holds.\n",
    "\n",
    "### In summary, checking the assumptions of linear regression is an important step in analyzing the results of a linear regression model. It helps to ensure that the results are valid and reliable. Diagnostic tests can be performed to check whether the assumptions hold in a given dataset. If the assumptions are violated, appropriate measures can be taken to address the issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a linear regression model, the slope represents the rate of change of the response variable (Y) with respect to the predictor variable (X). It indicates how much the response variable changes for a unit change in the predictor variable. The intercept represents the expected value of the response variable when the predictor variable is zero.\n",
    "\n",
    "### For example, consider a real-world scenario where you want to predict the sales of a company based on its advertising budget. You collect data on the advertising budget and corresponding sales for several months and fit a linear regression model. Let's say the model is:\n",
    "\n",
    "~~~\n",
    "Sales = 1000 + 5 * Advertising Budget\n",
    "\n",
    "In this model, the intercept of 1000 represents the expected sales when the advertising budget is zero. This might be due to other factors such as brand recognition, customer loyalty, or previous marketing efforts. The slope of 5 indicates that for every additional unit of advertising budget spent, sales are expected to increase by 5 units.\n",
    "\n",
    "So, for example, if the advertising budget is $10,000, the expected sales would be:\n",
    "\n",
    "Sales = 1000 + 5 * 10,000\n",
    "Sales = 51,000\n",
    "\n",
    "This means that the company is expected to make $51,000 in sales if they spend $10,000 on advertising, assuming other factors remain constant.\n",
    "~~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Slope and Intercept](https://www.katesmathlessons.com/uploads/1/6/1/0/1610286/1163738_orig.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : Explain the concept of gradient descent. How is it used in machine learning?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent is an iterative optimization algorithm used to minimize the cost function of a machine learning model. In simple terms, it is a way to adjust the parameters of a model to make it fit the training data better.\n",
    "\n",
    "### The idea behind gradient descent is to calculate the gradient of the cost function with respect to each parameter and move in the opposite direction of the gradient to find the minimum value of the cost function. The gradient is a vector that indicates the direction of the steepest ascent of the cost function. By moving in the opposite direction, we can descend to the minimum value of the cost function.\n",
    "\n",
    "### In machine learning, gradient descent is used to optimize the parameters of the model to minimize the error between the predicted values and the actual values. The cost function is a measure of the error between the predicted values and the actual values, and the goal is to minimize this cost function.\n",
    "\n",
    "### There are two types of gradient descent: batch gradient descent and stochastic gradient descent. In batch gradient descent, the gradient is calculated using the entire training dataset. In stochastic gradient descent, the gradient is calculated using only one example at a time.\n",
    "\n",
    "### Gradient descent is used in many machine learning algorithms, including linear regression, logistic regression, and neural networks. It is an essential part of machine learning because it allows us to find the optimal parameters of a model that best fit the training data, and therefore, make accurate predictions on new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation for Gradient Descent:\n",
    "$\\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$\n",
    "\n",
    "where $\\theta$ is a vector of the parameters of the model, $\\alpha$ is the learning rate, $J(\\theta)$ is the cost function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient Descent](https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example implementation of gradient descent in python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derrivatives calculation of gradient descent in Linear Regression\n",
    "\n",
    "1. Loss Function:\n",
    "\n",
    "$J(m,c) =\\frac{1}{n}\\displaystyle\\sum_{i=1} ^ {n} (y_i-(mx_i+c))^2$\n",
    "\n",
    "2. Partial Derrivative wrt intercept:\n",
    "\n",
    "$\\frac{\\partial J(m,c)}{\\partial c} = \\frac{-2}{n}\\displaystyle\\sum_{i=1} ^ {n} (y_i-(mx_i+c))$\n",
    "\n",
    "3. Partial Derrivative wrt slope :\n",
    "\n",
    "$\\frac{\\partial J(m,c)}{\\partial m} = \\frac{-2}{n}\\displaystyle\\sum_{i=1} ^ {n} (y_i-(mx_i+c))*x_i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 : Describe the multiple linear regression model. How does it differ from simple linear regression\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression is a statistical technique used to study the relationship between a dependent variable and two or more independent variables. It involves estimating the relationship between the dependent variable and multiple independent variables simultaneously. In this model, the dependent variable is predicted as a function of the independent variables.\n",
    "\n",
    "### The mathematical formula for multiple linear regression can be represented as:\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{y}$ is the predicted value of the dependent variable\n",
    "* $x_1$, $x_2$, ..., $x_k$ are the values of the independent variables for which we want to make a prediction\n",
    "* $\\beta_0$ is the intercept (the value of y when all independent variables are 0)\n",
    "* $\\beta_1$, $\\beta_2$, ..., $\\beta_k$ are the slopes (the change in y for a unit change in each independent variable) for each independent variable $x_1$, $x_2$, ..., $x_k$, respectively.\n",
    "\n",
    "### Multiple linear regression differs from simple linear regression in that it involves more than one independent variable. Simple linear regression only involves one independent variable and seeks to establish a linear relationship between the dependent variable and that independent variable. In contrast, multiple linear regression seeks to establish a relationship between the dependent variable and multiple independent variables simultaneously.\n",
    "\n",
    "### The advantage of multiple linear regression over simple linear regression is that it allows for the identification of multiple factors that contribute to the variation in the dependent variable. This can provide a more complete understanding of the relationship between the dependent variable and the independent variables, and can lead to more accurate predictions. However, multiple linear regression can also be more complex to interpret and may require more data to provide meaningful results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 : Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity refers to a situation in multiple linear regression where two or more predictor variables are highly correlated with each other. This can cause problems in the estimation of the regression coefficients, as the individual effects of the collinear variables on the response variable may be difficult to distinguish.\n",
    "\n",
    "### Multicollinearity can be detected by examining the correlation matrix between the predictor variables. A correlation coefficient close to 1 or -1 indicates a high degree of correlation between two variables, while a coefficient close to 0 indicates little correlation. Another way to detect multicollinearity is to look at the variance inflation factor (VIF), which measures the extent to which the variance of the estimated regression coefficients is inflated due to the multicollinearity. A VIF greater than 10 indicates a potential issue with multicollinearity.\n",
    "\n",
    "### Equation for VIF given as below :\n",
    "\n",
    "$VIF_j = \\frac{1}{1 - R_j^2}$\n",
    "\n",
    "where $VIF_j$ is the VIF for the j-th predictor variable, and $R_j^2$ is the coefficient of determination (R-squared) for the regression of the j-th predictor variable on all the other predictor variables.\n",
    "\n",
    "### To address multicollinearity, one approach is to remove one of the correlated predictor variables from the model. Another approach is to use dimensionality reduction techniques, such as principal component analysis (PCA), to transform the correlated predictor variables into a smaller set of uncorrelated variables. Additionally, regularization techniques such as Ridge or Lasso regression can help mitigate the impact of multicollinearity on the regression coefficients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 : Describe the polynomial regression model. How is it different from linear regression?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth-degree polynomial. In other words, it is a curve-fitting technique in which a polynomial function is used to approximate the relationship between the two variables.\n",
    "\n",
    "### The basic idea behind polynomial regression is to take the basic linear regression model, which fits a straight line to the data, and extend it by adding polynomial terms to the equation. These polynomial terms are added by including higher-order powers of the independent variable x, such as x², x³, etc.\n",
    "\n",
    "### Formula for Polynomial regression is :\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_n x^n$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\hat{y}$ is the predicted value of the dependent variable\n",
    "* $x$ is the values of the independent variables for which we want to make a prediction\n",
    "* $\\beta_0$ is the intercept (the value of y when all independent variables are 0)\n",
    "* $\\beta_1$, $\\beta_2$, ..., $\\beta_n$ are the slopes (the change in y for a unit change in each independent variable) for each polynomial degree $x$, $x^2$, ..., $x^n$, respectively.\n",
    "\n",
    "### So, while the linear regression model assumes a linear relationship between x and y, the polynomial regression model allows for a more flexible, nonlinear relationship between the two variables. By fitting a polynomial function to the data, the model can capture more complex patterns in the data that may not be captured by a simple linear model.\n",
    "\n",
    "### One of the main differences between linear regression and polynomial regression is that the former can only model linear relationships, while the latter can model nonlinear relationships as well. Another key difference is that the polynomial regression model can fit more complex patterns in the data, but it may also be more prone to overfitting if too many polynomial terms are included. Linear regression is often preferred when the relationship between the variables is expected to be linear, and when interpretability is more important than predictive power, while polynomial regression is more suitable when a nonlinear relationship is suspected, and predictive power is more important than interpretability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 : What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. In contrast, linear regression models the relationship between the variables as a straight line. Here are the advantages and disadvantages of polynomial regression compared to linear regression:\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. Polynomial regression can fit a wide range of complex nonlinear relationships between the independent and dependent variables.\n",
    "\n",
    "2. It can capture the curvature in the data better than linear regression and provide a better fit to the data.\n",
    "\n",
    "3. It can be more accurate than linear regression when the relationship between the variables is nonlinear.\n",
    "\n",
    "4. Polynomial regression can provide insights into the shape and nature of the relationship between the variables.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "1. Polynomial regression can easily overfit the data, especially when the degree of the polynomial is high, leading to poor generalization to new data.\n",
    "\n",
    "2. It can be computationally expensive, especially for high-degree polynomials and large datasets.\n",
    "\n",
    "3. Polynomial regression can be less interpretable than linear regression, as it involves fitting a more complex model.\n",
    "\n",
    "### In general, polynomial regression is preferred over linear regression when the relationship between the variables is nonlinear and cannot be well represented by a straight line. It is useful in situations where the data shows a curvature or a U-shape, for example. However, care must be taken to avoid overfitting the data by selecting an appropriate degree of the polynomial and by regularizing the model using techniques such as ridge regression or Lasso regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
